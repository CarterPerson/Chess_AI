## Carter Person, CS76 20F, PA3

### Description

The basis for my algorithms is the minimax algorithm. This algorithm has two inputs: the depth at which you would like the algorithm to search to, for which the larger the number you choose the longer the algorithm will take to make a move, but the better the move will be, and a boolean that states whether the "player" that is created is playing either black or white.

This algorithm only has one function that is called externally beyond the constructor, which is choose_move. This algorithm takes in the current state of the game and then checks all possible moves that can be made from that state. It checks these moves by iterating through all of the possible moves, changing the board to "make" the move, and then passing it on to the mini or the max function. If the "player" is playing black, then it will start with the mini function, and if the player is playing white then it will start with the max function. This is designed to make sure that the player will make the move that is optimal for them. After it is finished checking all children of that possible move, it then undoes the move to return to the initial board state, and then checks the next. Throughout this, it keeps track of the best move for the player, updating it if a better move is found.

The mini and max functions are extremely similar. In both, the first thing done is checking whether or not the current representation of the board represents a state in which the algorithm should stop searching deeper by checking if that game state is either terminating or at the maximum depth. Assuming that the algorithm should not keep searching, it then moves to find the best possible move, assuming that the opponent is acting rationally. In the mini and max functions, it iterates through all the possible moves similarly to choose_move. In the max function, it will run the mini function on all of the possible next board states, and return the score from the highest outcome of those. In the mini function, it acts similarly, but returns the lowest score from running the max function on all possible moves.

Note: if the depth limit is reached, the evaluate function is called. This function sums of all of the existing points from the white pieces (1 for pawn, 3 for bishop and knight, 5 for rook, 9 for queen, and then a random 600 value for king, just to ensure that it is both protected and targeted), and then subtracts the scores for all of the existing black pieces. This ensures that black is benefitted from protecting their pieces and taking white pieces, and the converse for white.


For the Alpha beta pruning adjustment, the mini and max functions pass their current highest/lowest score whenever they move on over to the max or mini functions. This value is then used to analyze any specific "branch" of the search tree. If, by using this value, it is determined that the existing branch will not be chosen by the above node as the best possible next move (for the mini function, if it finds a score lower than another child node of it's parent, meaning that the max function above it will not choose that move, regardless of what the other possibilities are), then the algorithm will not waste processing power on exploring subsequent options.


In the iterative deepening search method, which I only added to alphabeta pruning, You can see that, as the algorithm encounters different situations, oftentimes the algorithm will preference a different move as the depth increases. To test this, simply use the method iterative_deepening() on any alphaBetaAI object prior to it using get_move. This method is currently commented out in the test file, set to a time length of 10 seconds. This can be changed. For this search, I essentially copy pasted the alphabetaAI, and then adjusted the choose_move to be get_next, which was the same method, just without printouts. I also made all the methods be able to accomodate different max depths that are not found in the self object, and imported time. In my evaluation method, I put in a check for how much time has passed. If time has passed, I just return the score of all of the current nodes that are being explored, and then revert the "bestmove" object in the search back to the previous depths search. The reason why I chose to do this is that we don't know where in the search of the next depth level the algorithm is, so, unless it is reverted back to the previous depth's best move, it may very well give an inferior move. It is also essential to stop the algorithm inside the evaluate function, because otherwise it could reach a depth of 5 or 6, and still have a minute long search to perform.

